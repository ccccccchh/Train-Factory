ğŸŸ æ•´ä½“æµç¨‹

    git clone https://github.com/ccccccchh/Train-Factory.git

    cd ../../Train-Factory

    pip install -e .

    python src/webui.py


ğŸ¦„ 1.1 åŸºåº§æ¨¡å‹

     https://huggingface.co/

ğŸ¦ 1.2 æ•°æ®é›†æ”¶é›†

    ç¤ºä¾‹ https://github.com/ccccccchh/Train-Factory/blob/main/data/fintech.json
  
    æ”¾å…¥dataä¸‹
    å¹¶åœ¨æ•°æ®æ³¨å†Œæ–‡ä»¶æ·»åŠ ï¼Œå¦‚ä¸‹ç¤ºä¾‹ä»£ç ï¼š
  
      "fintech": {
      "file_name": "fintech.json",
      "columns": {
        "prompt": "instruction",
        "query": "input",
        "response": "output",
        "history": "history"
      }
    }
    
  
ğŸŒ 2.3 å¯åŠ¨ Web UI

    cd Train-Factory
    python src/webui.py


ğŸ¦“ 2.4 é…ç½®æ–‡ä»¶

    cutoff_len: 1024
    dataset: fintech,identity
    dataset_dir: data
    do_train: true
    finetuning_type: lora
    flash_attn: auto
    fp16: true
    gradient_accumulation_steps: 8
    learning_rate: 0.0002
    logging_steps: 5
    lora_alpha: 16
    lora_dropout: 0
    lora_rank: 8
    lora_target: q_proj,v_proj
    lr_scheduler_type: cosine
    max_grad_norm: 1.0
    max_samples: 1000
    num_train_epochs: 10.0
    optim: adamw_torch
    packing: false
    per_device_train_batch_size: 2
    plot_loss: true
    preprocessing_num_workers: 16
    report_to: none
    save_steps: 100
    stage: sft
    use_unsloth: true
    warmup_steps: 0
    

ğŸ”¢ 2.5 æ¨¡å‹é‡åŒ–

    æ¨¡å‹é‡åŒ–ï¼ˆModel Quantizationï¼‰æ˜¯ä¸€ç§å°†æ¨¡å‹çš„å‚æ•°å’Œè®¡ç®—ä»é«˜ç²¾åº¦ï¼ˆé€šå¸¸æ˜¯ 32 ä½æµ®ç‚¹æ•°ï¼ŒFP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚ 16 ä½æµ®ç‚¹æ•°ï¼ŒFP16ï¼Œæˆ–è€… 8 ä½æ•´æ•°ï¼ŒINT8ï¼‰çš„è¿‡ç¨‹ã€‚
    

ğŸ› ï¸ 2.6 æ¨¡å‹åˆå¹¶

    å°† base model ä¸è®­ç»ƒå¥½çš„ LoRA Adapter åˆå¹¶æˆä¸€ä¸ªæ–°çš„æ¨¡å‹ã€‚âš ï¸ ä¸è¦ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹æˆ– `quantization_bit` å‚æ•°æ¥åˆå¹¶ LoRA é€‚é…å™¨ã€‚
